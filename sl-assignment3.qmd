---
title: "Medical Insurance Cost Classification"
subtitle: "Supervised Learning - Assignment 3"
author: "Cesaire Tobias"
date: "May 30, 2025"
format: 
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    lof: true
    lot: true
    documentclass: article
    geometry: margin=1in
    fontsize: 11pt
    linestretch: 1.2
execute:
  echo: false
  warning: false
  message: false
---

```{r setup, include=FALSE}
# Load required libraries
library(knitr)       # For dynamic report generation
library(dplyr)       # For data manipulation
library(tidyr)       # For data tidying
library(ggplot2)     # For data visualization
library(gridExtra)   # For arranging multiple plots
library(kableExtra)  # For enhanced table formatting
library(scales)      # For formatting plot scales
library(corrplot)    # For correlation visualization
library(caret)       # For model training and evaluation
library(e1071)       # For SVM
library(nnet)        # For neural nets
library(pROC)        # For ROC curves
library(PRROC)       # For precision-recall curves
library(viridis)     # For better color palettes
library(NeuralNetTools)  # For network visualization

# Set seed for reproducibility
my_seed <- 9104

# Set default chunk options for the entire document
knitr::opts_chunk$set(
  echo = FALSE,        # Don't show code in final output
  warning = FALSE,     # Suppress warnings
  message = FALSE,     # Suppress messages
  fig.align = "center",  # Center figures
  fig.pos = "H",       # Position figures exactly here
  fig.width = 6.5,     # Set default figure width (adjusted to prevent overflow)
  fig.height = 4.5,    # Set default figure height (adjusted to prevent overflow)
  out.width = "95%",   # Set output figure width
  dpi = 300,           # Higher resolution
  results = "asis"     # Output results as-is
)

# Custom theme for consistent plot styling
my_theme <- theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5, color = "gray40"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.position = "bottom",
    legend.title = element_blank(),
    legend.text = element_text(size = 10),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(fill = NA, color = "gray90"),
    plot.margin = unit(c(1, 1, 1, 1), "cm") 
  )

# Custom color palettes
charge_colors <- c("#5ab4ac", "#d8b365")  # For binary target
model_colors <- viridis::viridis(4, option = "plasma")  # For model comparison
```

```{r load-data}
base_data_dir <- "./inst/extdata"
github_repo <- "https://github.com/ces0491/sl-assignment3.git"
github_data_url <- "https://github.com/ces0491/sl-assignment2/raw/refs/heads/main/data/insurance_data_A2.RData"

# Load training data
# Try to read the data from local files first
if (file.exists(file.path(base_data_dir, "insurance_A2.csv"))) {
  insurance_data <- read.csv(file.path(base_data_dir, "insurance_A2.csv"))
} else {
  # If local file doesn't exist, try to download from GitHub
  tryCatch({
    temp_file <- tempfile(fileext = ".RData")
    download.file(github_data_url, temp_file, mode = "wb")
    load(temp_file)
    # Assuming the RData file contains insurance_data object
    if (!exists("insurance_A2")) {
      stop("RData file doesn't contain insurance_data object")
    }
    unlink(temp_file)  # Clean up
    
    # Save the data locally for future runs
    if (!dir.exists(base_data_dir)) {
      dir.create(base_data_dir, recursive = TRUE)
    }
    write.csv(insurance_data, file.path(base_data_dir, "insurance_A2.csv"), row.names = FALSE)
  }, error = function(e) {
    stop("Error loading insurance data: ", e$message)
  })
}

# Initial inspection to verify data loaded correctly
if (nrow(insurance_data) == 0 || !("charges" %in% colnames(insurance_data))) {
  stop("Error loading insurance data")
}

# Load test data
test_data <- read.csv(file.path(base_data_dir, "A2_testing.csv"))

# Set seed for reproducibility
set.seed(my_seed)
```

# Introduction {#sec-introduction}

This report applies supervised machine learning classification techniques to predict whether medical insurance costs will be high or low based on patient characteristics. Two models are developed for this analysis - Support Vector Machine (SVM) and Neural Network.

Key questions addressed:

- **Patients**: Which personal factors significantly increase the likelihood of high insurance charges?
- **Insurers**: How effectively can machine learning models classify high vs. low insurance costs?
- **Policymakers**: Which factors should be targeted to reduce high-cost insurance claims?

Dataset features include:

- **age**: `Integer` - primary beneficiary's age
- **sex**: `Factor` - gender (female/male)
- **bmi**: `Continuous` - Body Mass Index
- **children**: `Integer` - number of dependents
- **smoker**: `Factor` - smoking status (yes/no)
- **region**: `Factor` - US residential area (northeast, southeast, southwest, northwest)

The target variable **charges** has been transformed (external to this analysis) from a continuous dollar amount to binary ("high"/"low").

# Data Import and Preparation {#sec-data-preparation}

```{r data-preparation}
# Check for missing values in both datasets
train_missing <- sum(is.na(insurance_data))
test_missing <- sum(is.na(test_data))

# Convert appropriate columns to factors
insurance_data <- insurance_data %>%
  dplyr::mutate(
    sex = as.factor(sex),
    smoker = as.factor(smoker),
    region = as.factor(region),
    charges = as.factor(charges),
    children = as.integer(children)
  )

# Explicitly set reference levels for consistent encoding
insurance_data$charges <- relevel(insurance_data$charges, ref = "low")
insurance_data$sex <- relevel(insurance_data$sex, ref = "female")
insurance_data$smoker <- relevel(insurance_data$smoker, ref = "no")
insurance_data$region <- relevel(insurance_data$region, ref = "northeast")

```

## Data Sources {#sec-data-sources}

The data can be accessed from:

- **GitHub Repository**: [sl-assignment3](https://github.com/ces0491/sl-assignment3.git)
- **Direct RData Link**: [insurance_data_A2.RData](https://github.com/ces0491/sl-assignment3/raw/refs/heads/main/data/insurance_data_A2.RData)

The dataset consists of `r nrow(insurance_data)` observations with `r train_missing` missing values in the training data and `r test_missing` missing values in the test data, indicating complete datasets.

## Methodology Overview {#sec-methodology}

The model development approach comprises three phases:

1. **Training Phase**: The `insurance_A2.csv` dataset is split into training (80%) and internal validation (20%) sets.
2. **Model Selection Phase**: Models are evaluated on the internal validation set with emphasis on the F1 score.
3. **External Validation Phase**: The best model is then applied to a separate dataset (`A2_testing.csv`).

# Exploratory Data Analysis {#sec-exploratory-data-analysis}

## Data Structure and Target Distribution {#sec-target-distribution}

```{r target-distribution, fig.height=5}
# Calculate the distribution of the target variable
charges_distribution <- table(insurance_data$charges)
charges_percent <- prop.table(charges_distribution) * 100
```

The target variable shows class distribution with `r round(charges_percent["low"], 1)`% "low" and `r round(charges_percent["high"], 1)`% "high" charges. This relatively balanced distribution means that standard evaluation metrics like accuracy are appropriate, though we will still prioritize the F1 score as it balances precision and recall.

## Variable Distributions and Relationships {#sec-variable-distributions}

```{r numerical-distributions, fig.cap="Distribution of Numerical Variables", fig.width=9, fig.height=6}
# Create histograms for numerical variables
p1 <- ggplot2::ggplot(insurance_data, ggplot2::aes(x = age)) +
  ggplot2::geom_histogram(bins = 20, fill = charge_colors[1], color = "white") +
  ggplot2::labs(x = "Age", y = "Frequency") +
  ggplot2::ggtitle("Age Distribution") +
  my_theme

p2 <- ggplot2::ggplot(insurance_data, ggplot2::aes(x = bmi)) +
  ggplot2::geom_histogram(bins = 20, fill = charge_colors[1], color = "white") +
  ggplot2::labs(x = "BMI", y = "Frequency") +
  ggplot2::ggtitle("BMI Distribution") +
  my_theme

p3 <- ggplot2::ggplot(insurance_data, ggplot2::aes(x = as.factor(children))) +
  ggplot2::geom_bar(fill = charge_colors[1]) +
  ggplot2::labs(x = "Number of Children", y = "Frequency") +
  ggplot2::ggtitle("Children Distribution") +
  my_theme

# Calculate smoker proportions
smoker_props <- insurance_data %>%
  dplyr::group_by(smoker, charges) %>%
  dplyr::summarise(count = dplyr::n(), .groups = "drop") %>%
  dplyr::group_by(smoker) %>%
  dplyr::mutate(
    prop = count / sum(count),
    percentage = prop * 100,
    formatted_pct = paste0(round(percentage, 1), "%")
  )

# Extract key values for dynamic text references
smoker_high_pct <- smoker_props %>% 
  dplyr::filter(smoker == "yes" & charges == "high") %>% 
  dplyr::pull(percentage)

smoker_low_pct <- smoker_props %>% 
  dplyr::filter(smoker == "yes" & charges == "low") %>% 
  dplyr::pull(percentage)

nonsmoker_high_pct <- smoker_props %>% 
  dplyr::filter(smoker == "no" & charges == "high") %>% 
  dplyr::pull(percentage)

nonsmoker_low_pct <- smoker_props %>% 
  dplyr::filter(smoker == "no" & charges == "low") %>% 
  dplyr::pull(percentage)

p4 <- ggplot2::ggplot(insurance_data, ggplot2::aes(x = smoker, fill = charges)) +
  ggplot2::geom_bar(position = "fill", width = 0.7, color = "white") +
  ggplot2::geom_text(ggplot2::aes(label = scales::percent(..count../tapply(..count.., ..x.., sum)[..x..])),
            position = ggplot2::position_fill(vjust = 0.5),
            stat = "count", 
            color = "white", 
            size = 4.5,
            fontface = "bold") +
  ggplot2::labs(title = "Proportion of Charges by Smoking Status",
       x = "Smoker",
       y = "Proportion") +
  ggplot2::scale_fill_manual(values = charge_colors) +
  ggplot2::scale_y_continuous(labels = scales::percent, expand = c(0, 0)) +
  my_theme

gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)
```

- **Age**: Fairly uniform across the adult age range (18-65)
- **BMI**: Centered around 25-35, with most values in the overweight to obese range
- **Children**: Highly skewed, with most individuals having 0-2 children
- **Smoking Status**: Shows a dramatic relationship with charges - `r round(smoker_high_pct, 1)`% of smokers are classified as "high" charges compared to only `r round(nonsmoker_high_pct, 1)`% of non-smokers

```{r correlation-matrix, fig.cap="Correlation Matrix of Variables", fig.height=5}
# Convert target to numeric for correlation
insurance_numeric <- insurance_data
# Ensure consistent encoding - "high" = 1, "low" = 0
insurance_numeric$charges_numeric <- ifelse(insurance_data$charges == "high", 1, 0)
insurance_numeric$smoker_numeric <- ifelse(insurance_data$smoker == "yes", 1, 0)
insurance_numeric$sex_numeric <- ifelse(insurance_data$sex == "male", 1, 0)

# Create correlation matrix
corr_vars <- c("age", "bmi", "children", "smoker_numeric", "sex_numeric", "charges_numeric")
correlation_matrix <- cor(insurance_numeric[, corr_vars])

```

**Correlation analysis indicates:**

- **Age** has the strongest correlation with high charges (`r round(correlation_matrix["age", "charges_numeric"], 2)`)
- **Smoking status** has the second strongest correlation with high charges (`r round(correlation_matrix["smoker_numeric", "charges_numeric"], 2)`)
- **BMI** shows moderate positive correlation (`r round(correlation_matrix["bmi", "charges_numeric"], 2)`)
- **Children** and **Sex** show weaker correlations

```{r feature-target-relationships, fig.cap="Relationships Between Features and Target Class", fig.width=9, fig.height=6}
# Create visualizations to explore relationship between features and target class
p1 <- ggplot2::ggplot(insurance_data, ggplot2::aes(x = charges, y = age, fill = charges)) +
  ggplot2::geom_boxplot() +
  ggplot2::labs(x = "Charges Class", y = "Age") +
  ggplot2::scale_fill_manual(values = charge_colors) +
  ggplot2::theme(legend.position = "none") +
  ggplot2::ggtitle("Age by Charges Class") +
  my_theme

p2 <- ggplot2::ggplot(insurance_data, ggplot2::aes(x = charges, y = bmi, fill = charges)) +
  ggplot2::geom_boxplot() +
  ggplot2::labs(x = "Charges Class", y = "BMI") +
  ggplot2::scale_fill_manual(values = charge_colors) +
  ggplot2::theme(legend.position = "none") +
  ggplot2::ggtitle("BMI by Charges Class") +
  my_theme

p3 <- ggplot2::ggplot(insurance_data, ggplot2::aes(x = charges, fill = region)) +
  ggplot2::geom_bar(position = "fill") +
  ggplot2::labs(x = "Charges Class", y = "Proportion") +
  ggplot2::scale_fill_manual(values = colorRampPalette(charge_colors)(4)) +
  ggplot2::ggtitle("Region by Charges Class") +
  my_theme

p4 <- ggplot2::ggplot(insurance_data, ggplot2::aes(x = charges, fill = sex)) +
  ggplot2::geom_bar(position = "fill") +
  ggplot2::labs(x = "Charges Class", y = "Proportion") +
  ggplot2::scale_fill_manual(values = charge_colors) +
  ggplot2::ggtitle("Sex by Charges Class") +
  my_theme

gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)
```

- **Age**: Individuals with high charges tend to be older (median age `r round(median(insurance_data$age[insurance_data$charges == "high"]), 1)` vs. `r round(median(insurance_data$age[insurance_data$charges == "low"]), 1)` for low charges)
- **BMI**: Higher BMI is associated with high charges (median BMI `r round(median(insurance_data$bmi[insurance_data$charges == "high"]), 1)` vs. `r round(median(insurance_data$bmi[insurance_data$charges == "low"]), 1)` for low charges)
- **Region**: Modest variations in charges across regions
- **Sex**: Minimal difference in charges classification between males and females

```{r smoker-bmi-interaction, fig.cap="Interaction Between Smoking Status and BMI", fig.width=8, fig.height=5}
# Explore the interaction between smoking and BMI

# Create a scatter plot with smooth line showing probability of high charges by BMI and smoking status
ggplot2::ggplot(insurance_data, ggplot2::aes(x = bmi, y = as.numeric(charges == "high"), color = smoker)) +
  ggplot2::geom_point(alpha = 0.7, position = ggplot2::position_jitter(height = 0.05, width = 0.3)) +
  ggplot2::geom_smooth(method = "loess", se = TRUE) +
  ggplot2::labs(
    x = "BMI", 
    y = "Probability of High Charges", 
    color = "Smoker"
  ) +
  ggplot2::scale_color_manual(values = c(charge_colors[1], charge_colors[2])) +
  ggplot2::scale_y_continuous(breaks = c(0, 1), labels = c("Low", "High")) +
  ggplot2::ggtitle("BMI vs. Charges by Smoking Status") +
  my_theme
```

This plot reveals that BMI has a stronger effect on charges for non-smokers than for smokers. While non-smokers have a higher probability of high charges as BMI increases, smokers have a high probability of high charges regardless of BMI.

## Key Findings from EDA {#sec-key-findings}

1. **Target Distribution**: Reasonably balanced (`r round(charges_percent["high"], 1)`% "high" and `r round(charges_percent["low"], 1)`% "low").

2. **Important Predictors**:
   - **Smoking**: The strongest predictor of high charges with `r round(smoker_high_pct, 1)`% of smokers having high charges
   - **Age**: Older individuals tend to have higher charges
   - **BMI**: Higher BMI is associated with higher charges (for non-smokers)

3. **Data Quality**:
   - No missing values
   - Some outliers present, particularly in BMI
   - Interactions between predictors suggest non-linear modeling approaches

These insights will guide our modeling approach, particularly the need to capture interactions between features. Support Vector Machines with non-linear kernels and Neural Networks are well-suited for capturing these complex patterns.

# Data Preprocessing {#sec-preprocessing}

1. **Data Splitting**: 80% training, 20% validation
2. **Feature Scaling**: Standardizing numerical features (mean = 0, sd = 1)
3. **Categorical Encoding**: Converting categorical variables to dummy variables for neural network compatibility
4. **Target Encoding**: Converting "high"/"low" targets to 1/0 for neural network compatibility

```{r preprocessing}
# Split data into training and validation sets
set.seed(my_seed)

train_index <- caret::createDataPartition(insurance_data$charges, p = 0.8, list = FALSE)
train_data <- insurance_data[train_index, ]
valid_data <- insurance_data[-train_index, ]

# Create feature matrices and target vectors
X_train <- train_data %>% dplyr::select(-charges)
y_train <- train_data$charges
X_valid <- valid_data %>% dplyr::select(-charges)
y_valid <- valid_data$charges

# Verify encoding consistency in split data
stopifnot(
  all(levels(train_data$charges) == levels(insurance_data$charges)),
  all(levels(valid_data$charges) == levels(insurance_data$charges))
)

# Store class distribution for consistent referencing
training_charges_dist <- table(train_data$charges)
training_charges_percent <- prop.table(training_charges_dist) * 100

# Create preprocessing recipe for consistent preprocessing
preprocess_recipe <- caret::preProcess(
  X_train,
  method = c("center", "scale"),  # Standardize numerical features
  verbose = FALSE
)

# Apply preprocessing to training and validation sets
X_train_scaled <- predict(preprocess_recipe, X_train)
X_valid_scaled <- predict(preprocess_recipe, X_valid)

# Apply the same preprocessing to test data
X_test_scaled <- predict(preprocess_recipe, test_data)

# Create dummy variables for categorical features (for neural network)
dummies_model <- caret::dummyVars(" ~ .", data = X_train)
X_train_dummy <- stats::predict(dummies_model, X_train)
X_valid_dummy <- stats::predict(dummies_model, X_valid)
X_test_dummy <- stats::predict(dummies_model, test_data)

# Convert to matrices for model compatibility
X_train_matrix <- as.matrix(X_train_dummy)
X_valid_matrix <- as.matrix(X_valid_dummy)
X_test_matrix <- as.matrix(X_test_dummy)

# Prepare target variables for numerical encoding
y_train_numeric <- as.numeric(y_train == "high")
y_valid_numeric <- as.numeric(y_valid == "high")
```

After preprocessing, we have `r nrow(X_train_scaled)` training samples and `r nrow(X_valid_scaled)` validation samples, with `r ncol(X_train_matrix)` features after one-hot encoding.

# Modeling {#sec-modeling}

## Support Vector Machine (SVM) {#sec-svm}

Support Vector Machines are well-suited for this classification task due to their ability to find complex decision boundaries using kernel functions. They're particularly effective when:

1. The relationship between features and target is non-linear
2. The dimensionality is moderate (as in our case)
3. The decision boundary between classes is complex

```{r svm-implementation}
# Set seed for reproducibility
set.seed(my_seed)

# Make sure factors have consistent levels
y_train <- factor(y_train, levels = c("low", "high"))
y_valid <- factor(y_valid, levels = c("low", "high"))

# Create a data frame with both predictors and response for training
train_df <- cbind(X_train_scaled, charges = y_train)

# Define parameter grid
gamma_values <- c(0.01, 0.1, 1)
cost_values <- c(1, 10, 100)

# Initialize results dataframe
tune_results <- data.frame(
  gamma = numeric(),
  cost = numeric(),
  accuracy = numeric(),
  sensitivity = numeric(),
  specificity = numeric(),
  precision = numeric(),
  f1 = numeric(),
  ROC = numeric(),
  stringsAsFactors = FALSE
)

for (gamma_val in gamma_values) {
  for (cost_val in cost_values) {
    # Use try-catch for each parameter combination to handle potential errors
    result <- try({
      # Train model with current parameters passed in loop
      temp_model <- svm(
        formula = charges ~ .,
        data = train_df,
        kernel = "radial", 
        gamma = gamma_val,
        cost = cost_val,
        probability = TRUE
      )
      
      # Predict on validation data
      temp_pred <- predict(temp_model, X_valid_scaled)
      
      # Calculate metrics
      temp_cm <- confusionMatrix(temp_pred, y_valid, positive = "high")
      
      # Add results to dataframe
      new_row <- data.frame(
        gamma = gamma_val,
        cost = cost_val,
        accuracy = temp_cm$overall["Accuracy"],
        sensitivity = temp_cm$byClass["Sensitivity"],
        specificity = temp_cm$byClass["Specificity"],
        precision = temp_cm$byClass["Pos Pred Value"],
        f1 = temp_cm$byClass["F1"],
        ROC = (temp_cm$byClass["Sensitivity"] + temp_cm$byClass["Specificity"]) / 2
      )
      new_row
    }, silent = TRUE)
    
    # If successful, add to results
    if (!inherits(result, "try-error")) {
      tune_results <- rbind(tune_results, result)
    }
  }
}

# Find best parameters by F1 score
if (nrow(tune_results) > 0) {
  best_params <- tune_results[which.max(tune_results$f1), ]
  
  # Store best parameters for later use
  best_gamma <- best_params$gamma
  best_cost <- best_params$cost
  
  # Train final model with best parameters
  final_svm_model <- svm(
    formula = charges ~ .,
    data = train_df,
    kernel = "radial",
    gamma = best_gamma,
    cost = best_cost,
    probability = TRUE
  )
  
  # Make predictions
  svm_pred <- predict(final_svm_model, X_valid_scaled)
  svm_probs <- predict(final_svm_model, X_valid_scaled, probability = TRUE)
  svm_pred_prob <- attr(svm_probs, "probabilities")[, "high"]
  
  # Calculate performance metrics
  cm <- confusionMatrix(svm_pred, y_valid, positive = "high")
  svm_results <- data.frame(
    accuracy = cm$overall["Accuracy"],
    sensitivity = cm$byClass["Sensitivity"],
    specificity = cm$byClass["Specificity"],
    f1 = cm$byClass["F1"]
  )
}
```

```{r svm-tune-results-table}
# Create a formatted table for SVM tuning results
tune_results_table <- tune_results %>%
  arrange(desc(f1)) %>%
  mutate(across(where(is.numeric), ~round(., 3))) %>% 
  top_n(5)

kableExtra::kbl(
  tune_results_table,
  caption = "SVM Hyperparameter Tuning Results",
  format = "latex",
  booktabs = TRUE,
  align = "c"
) %>%
  kableExtra::kable_styling(
    latex_options = c("HOLD_position", "scale_down"),
    font_size = 9,
    position = "center"
  ) %>%
  kableExtra::row_spec(1, bold = TRUE, background = "#E8F4F9")
```

### SVM Model Specification and Justification {#sec-svm-justification}

Based on our EDA and tuning results, we selected an SVM model with the following parameters:

- **Kernel**: Radial Basis Function (RBF)
  - **Justification**: The RBF kernel can capture complex non-linear decision boundaries, which is appropriate given the interactions we observed between features (e.g., BMI and smoking status) as found in the EDA.

- **Cost = `r best_cost`**
  - **Justification**: This regularization parameter balances between maximizing the margin and minimizing classification error. Our tuning results show that C = `r best_cost` provides the best trade-off. A higher C value means we prioritize correctly classifying training points over having a wider margin.

- **Gamma = `r best_gamma`**
  - **Justification**: Gamma defines how far the influence of a single training example reaches. With gamma = `r best_gamma`, our model captures the right balance between local and global patterns in the data.

The best configuration achieved an accuracy of `r round(svm_results$accuracy[1], 3)`, a sensitivity of `r round(svm_results$sensitivity[1], 3)`, and a specificity of `r round(svm_results$specificity[1], 3)` on the training data.

## Neural Network {#sec-nn}

Neural networks can learn complex patterns and non-linear relationships, making them suitable for our insurance cost classification task. We'll implement a feedforward neural network using the `nnet` package, which provides a more stable implementation for R than keras which requires Python libraries.

### Neural Network Architecture Design {#sec-nn-design}

```{r nn-design, fig.cap="Neural Network Architecture (Untuned)"}
set.seed(my_seed)

# Prepare data for nnet (combine features and target)
# Use the scaled data to match the preprocessing of the SVM model
X_train_df <- as.data.frame(X_train_scaled)
X_valid_df <- as.data.frame(X_valid_scaled)

train_nnet <- cbind(X_train_df, charges_numeric = ifelse(y_train == "high", 1, 0))
valid_nnet <- cbind(X_valid_df, charges_numeric = ifelse(y_valid == "high", 1, 0))

# Create a visual representation of the neural network architecture using arbitrary pre-tuned values
viz_nn_model <- nnet(
  charges_numeric ~ ., 
  data = train_nnet, 
  size = 10,  # Middle value from our tuning grid
  decay = 0.01,
  maxit = 500,
  linout = FALSE,
  trace = FALSE
)

# Create a named vector for input features
feature_names <- colnames(X_train_scaled)

# Plot the neural network with custom styling
par(mar = c(0, 0, 2, 0))  # Adjust margins
plotnet(
  viz_nn_model,
  pos_col = charge_colors[1],  # Positive weights
  neg_col = charge_colors[2],  # Negative weights
  bias = TRUE,                # Show bias terms
  circle_cex = 3,             # Size of nodes
  cex_val = 0.7,              # Text size
  alpha_val = 0.7,            # Transparency
  circle_col = "lightblue",   # Node color
  custom_labels = c(feature_names, "", "High Charges"),
  layer_label = TRUE,
  main = "Neural Network Model Architecture"
)
```

For our neural network, we design a feedforward architecture with one hidden layer. This design can capture complex non-linear relationships in the data, including interactions between variables like smoking status and BMI.

```{r nn-tuning}

# Set up grid for tuning
nn_params <- expand.grid(
  size = c(5, 10, 15),          # Number of units in hidden layer
  decay = c(0.001, 0.01, 0.1),  # Weight decay for regularization
  maxit = 500                    # Maximum iterations
)

# Cross-validation for each parameter set
nn_results <- data.frame()

# Loop through parameter combinations
set.seed(my_seed)
for (i in 1:nrow(nn_params)) {
  # Get current parameter set
  size <- nn_params$size[i]
  decay <- nn_params$decay[i]
  maxit <- nn_params$maxit[i]
  
  # Create cross-validation folds
  cv_folds <- createFolds(train_nnet$charges_numeric, k = 3)
  
  # Initialize metrics for this parameter set
  fold_metrics <- data.frame()
  
  # Cross-validation
  for (fold in names(cv_folds)) {
    # Create train/validation split
    fold_train <- train_nnet[-cv_folds[[fold]], ]
    fold_valid <- train_nnet[cv_folds[[fold]], ]
    
    # Train model
    model <- nnet(
      charges_numeric ~ ., 
      data = fold_train, 
      size = size,
      decay = decay,
      maxit = maxit,
      linout = FALSE,   # Use logistic output
      trace = FALSE     # Don't print progress
    )
    
    # Make predictions
    pred_prob <- predict(model, newdata = fold_valid[, -ncol(fold_valid)])
    pred_class <- ifelse(pred_prob > 0.5, 1, 0)
    
    # Calculate metrics
    cm <- confusionMatrix(
      factor(pred_class, levels = c(0, 1)),
      factor(fold_valid$charges_numeric, levels = c(0, 1)),
      positive = "1"
    )
    
    # Store metrics
    fold_metrics <- rbind(fold_metrics, data.frame(
      size = size,
      decay = decay,
      maxit = maxit,
      accuracy = cm$overall["Accuracy"],
      sensitivity = cm$byClass["Sensitivity"],
      specificity = cm$byClass["Specificity"],
      precision = cm$byClass["Pos Pred Value"],
      f1 = cm$byClass["F1"]
    ))
  }
  
  # Average metrics across folds
  avg_metrics <- colMeans(fold_metrics[, c("accuracy", "sensitivity", "specificity", "precision", "f1")])
  
  # Add to results
  nn_results <- rbind(nn_results, data.frame(
    size = size,
    decay = decay,
    maxit = maxit,
    accuracy = avg_metrics["accuracy"],
    sensitivity = avg_metrics["sensitivity"],
    specificity = avg_metrics["specificity"],
    precision = avg_metrics["precision"],
    f1 = avg_metrics["f1"]
  ))
}

# Find best parameters
best_nn_params <- nn_results %>% dplyr::arrange(desc(f1)) %>% head(1)

# Display tuning results
kableExtra::kbl(
  nn_results %>% 
    dplyr::arrange(desc(f1)) %>% 
    dplyr::select(-accuracy) %>% 
    dplyr::top_n(5),
  caption = "Neural Network Hyperparameter Tuning Results",
  format = "latex",
  booktabs = TRUE,
  align = "c",
  digits = 3
) %>%
  kableExtra::kable_styling(
    latex_options = c("HOLD_position", "scale_down"),
    font_size = 9,
    position = "center"
  ) %>%
  kableExtra::row_spec(1, bold = TRUE, background = "#E8F4F9")
```

```{r nn-final}
# Build final model with best parameters
set.seed(my_seed)

final_nn_model <- nnet(
  charges_numeric ~ ., 
  data = train_nnet, 
  size = best_nn_params$size,
  decay = best_nn_params$decay,
  maxit = best_nn_params$maxit,
  linout = FALSE,
  trace = FALSE
)

# Make predictions on validation set
nn_pred_prob <- predict(final_nn_model, newdata = valid_nnet[, -ncol(valid_nnet)])
nn_pred <- ifelse(nn_pred_prob > 0.5, "high", "low")
nn_pred <- factor(nn_pred, levels = c("low", "high"))

```

```{r nn-performance-plot, fig.cap="Neural Network Classification Performance", fig.width=10, fig.height=4}
# Create a dataset for visualization
pred_data <- data.frame(
  actual = y_valid,
  predicted = nn_pred,
  probability = nn_pred_prob
)

# Plot 1: Predicted probabilities distribution by actual class
p1 <- ggplot2::ggplot(pred_data, ggplot2::aes(x = probability, fill = actual)) +
  ggplot2::geom_histogram(alpha = 0.7, bins = 30, position = "identity") +
  ggplot2::geom_vline(xintercept = 0.5, linetype = "dashed", color = "black") +
  ggplot2::scale_fill_manual(values = charge_colors) +
  ggplot2::labs(title = "Distribution of Predicted Probabilities",
                x = "Predicted Probability of High Charges",
                y = "Count") +
  my_theme

# Plot 2: Accuracy at different probability thresholds
thresholds <- seq(0.1, 0.9, by = 0.05)
accuracy_at_threshold <- sapply(thresholds, function(t) {
  preds <- ifelse(pred_data$probability > t, "high", "low")
  preds <- factor(preds, levels = c("low", "high"))
  mean(preds == pred_data$actual)
})

p2 <- ggplot2::ggplot(data.frame(threshold = thresholds, accuracy = accuracy_at_threshold),
                      ggplot2::aes(x = threshold, y = accuracy)) +
  ggplot2::geom_line(color = charge_colors[1], size = 1.2) +
  ggplot2::geom_point(color = charge_colors[2], size = 3) +
  ggplot2::geom_vline(xintercept = 0.5, linetype = "dashed") +
  ggplot2::labs(title = "Accuracy vs. Probability Threshold",
                x = "Classification Threshold",
                y = "Accuracy") +
  ggplot2::scale_y_continuous(limits = c(0.7, 1)) +
  my_theme

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

### Neural Network Architecture and Justification {#sec-nn-justification}

Our neural network architecture consists of:

1. **Input Layer**: `r ncol(X_train_scaled)` neurons (matching our feature dimensionality)

2. **Hidden Layer**: `r best_nn_params$size` neurons with sigmoid activation
   
   **Justification**: This structure allows the model to learn non-linear patterns in the data. The optimal number of neurons was determined through cross-validation to balance between underfitting and overfitting.

3. **Regularization**: 
   - Weight decay of `r best_nn_params$decay` to prevent overfitting by penalizing large weights
   - Early stopping criteria by limiting to `r best_nn_params$maxit` iterations
   
   **Justification**: These regularization techniques help prevent the model from memorizing the training data, instead encouraging it to learn generalizable patterns.

4. **Output Layer**: 1 neuron with sigmoid activation for binary classification
   
   **Justification**: The sigmoid activation constrains the output between 0 and 1, representing the probability of the "high" charges class.

The performance visualization shows how the model's predictions are distributed and how accuracy varies with different classification thresholds. The default threshold of 0.5 provides a good balance, but adjusting this could optimize for different business objectives (e.g., prioritizing recall over precision).

# Model Evaluation and Comparison {#sec-evaluation}

To evaluate our models, we'll use the metrics accuracy, precision, recall, F1 score, and ROC AUC.

```{r model-evaluation}
# Function to calculate all metrics
calculate_metrics <- function(actual, predicted, predicted_prob) {
  # Ensure factors have consistent levels
  actual <- factor(actual, levels = c("low", "high"))
  predicted <- factor(predicted, levels = c("low", "high"))
  
  # Create confusion matrix
  cm <- caret::confusionMatrix(predicted, actual, positive = "high")
  
  # Calculate ROC
  roc_obj <- pROC::roc(ifelse(actual == "high", 1, 0), 
                       predicted_prob, 
                       quiet = TRUE)
  auc_roc <- pROC::auc(roc_obj)
  
  # Calculate precision-recall curve and AUC
  actual_numeric <- ifelse(actual == "high", 1, 0)
  
  # Create PR curve
  pr_curve <- PRROC::pr.curve(
    scores.class0 = predicted_prob[actual_numeric == 1], 
    scores.class1 = predicted_prob[actual_numeric == 0],
    curve = FALSE
  )
  auc_pr <- pr_curve$auc.integral
  
  # Return all metrics
  return(list(
    accuracy = cm$overall["Accuracy"],
    precision = cm$byClass["Pos Pred Value"],
    recall = cm$byClass["Sensitivity"],
    f1 = cm$byClass["F1"],
    auroc = auc_roc,
    auprc = auc_pr,
    confusion = cm$table
  ))
}

# Calculate metrics for both models
svm_metrics <- calculate_metrics(y_valid, svm_pred, svm_pred_prob)
nn_metrics <- calculate_metrics(y_valid, nn_pred, nn_pred_prob)

# Create metrics table
metrics_table <- data.frame(
  Metric = c("Accuracy", "F1 Score", "Recall", "Precision", "AUROC", "AUPRC"),
  SVM = c(
    paste0(round(svm_metrics$accuracy * 100, 1), "%"),
    paste0(round(svm_metrics$f1 * 100, 1), "%"),
    paste0(round(svm_metrics$recall * 100, 1), "%"),
    paste0(round(svm_metrics$precision * 100, 1), "%"),
    round(svm_metrics$auroc, 3),
    round(svm_metrics$auprc, 3)
  ),
  NN = c(
    paste0(round(nn_metrics$accuracy * 100, 1), "%"),
    paste0(round(nn_metrics$f1 * 100, 1), "%"),
    paste0(round(nn_metrics$recall * 100, 1), "%"),
    paste0(round(nn_metrics$precision * 100, 1), "%"),
    round(nn_metrics$auroc, 3),
    round(nn_metrics$auprc, 3)
  )
)

# Determine which model has better F1 score
better_model <- ifelse(svm_metrics$f1 > nn_metrics$f1, "SVM", "Neural Network")
better_f1 <- max(svm_metrics$f1, nn_metrics$f1)

# Create numeric versions for row highlighting
numeric_metrics <- data.frame(
  SVM = c(
    svm_metrics$accuracy * 100,
    svm_metrics$f1 * 100,
    svm_metrics$recall * 100,
    svm_metrics$precision * 100,
    svm_metrics$auroc,
    svm_metrics$auprc
  ),
  NN = c(
    nn_metrics$accuracy * 100,
    nn_metrics$f1 * 100,
    nn_metrics$recall * 100,
    nn_metrics$precision * 100,
    nn_metrics$auroc,
    nn_metrics$auprc
  )
)

kableExtra::kbl(
  metrics_table,
  caption = "Model Performance Comparison",
  format = "latex",
  booktabs = TRUE,
  align = "c",
  col.names = c("Metric", "SVM", "Neural Network")
) %>%
  kableExtra::kable_styling(
    latex_options = c("HOLD_position", "scale_down"),  # Add scale_down
    font_size = 9,
    position = "center",
    full_width = FALSE
  ) %>%
  kableExtra::row_spec(which(numeric_metrics$SVM > numeric_metrics$NN), 
                     background = "#E1F5FE") %>%
  kableExtra::row_spec(which(numeric_metrics$NN > numeric_metrics$SVM), 
                     background = "#E8F5E9") %>%
  kableExtra::footnote(
    general = "Color key: Blue = SVM performs better; Green = Neural Network performs better.",
    general_title = "Note: ",
    threeparttable = TRUE,  # This ensures footnotes stay within table width
    footnote_as_chunk = TRUE
  )
```

```{r roc-curves, fig.cap="ROC Curves Comparison", fig.width=9, fig.height=7}
# Create ROC curves for visual comparison
roc_svm <- pROC::roc(ifelse(y_valid == "high", 1, 0), svm_pred_prob, quiet = TRUE)
roc_nn <- pROC::roc(ifelse(y_valid == "high", 1, 0), nn_pred_prob, quiet = TRUE)

# Plot ROC curves
roc_data <- data.frame(
  FPR_SVM = 1 - roc_svm$specificities,
  TPR_SVM = roc_svm$sensitivities,
  FPR_NN = 1 - roc_nn$specificities,
  TPR_NN = roc_nn$sensitivities
)

ggplot2::ggplot() +
  ggplot2::geom_line(data = roc_data, ggplot2::aes(x = FPR_SVM, y = TPR_SVM, color = "SVM"), size = 1.2) +
  ggplot2::geom_line(data = roc_data, ggplot2::aes(x = FPR_NN, y = TPR_NN, color = "Neural Network"), size = 1.2) +
  ggplot2::geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  ggplot2::labs(
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)",
    title = "ROC Curves Comparison"
  ) +
  ggplot2::scale_color_manual(values = c("SVM" = charge_colors[1], "Neural Network" = charge_colors[2])) +
  ggplot2::coord_equal() +
  ggplot2::theme(legend.title = ggplot2::element_blank()) +
  my_theme +
  ggplot2::annotate("text", x = 0.5, y = 0.25, 
           label = paste("SVM AUC:", round(svm_metrics$auroc, 3)), 
           color = charge_colors[1], hjust = 0) +
  ggplot2::annotate("text", x = 0.5, y = 0.2, 
           label = paste("NN AUC:", round(nn_metrics$auroc, 3)), 
           color = charge_colors[2], hjust = 0)
```

```{r confusion-matrices, fig.cap="Confusion Matrices", fig.width=8, fig.height=4}
# Function to create confusion matrix visualization
plot_confusion_matrix <- function(conf_matrix, title) {
  # Convert to data frame format
  cm_data <- as.data.frame(conf_matrix)
  names(cm_data) <- c("Predicted", "Actual", "Freq")
  
  # Calculate percentages
  total <- sum(cm_data$Freq)
  cm_data$Percentage <- round(cm_data$Freq / total * 100, 1)
  cm_data$Label <- paste0(cm_data$Freq, "\n(", cm_data$Percentage, "%)")
  cm_data$IsCorrect <- cm_data$Predicted == cm_data$Actual
  
  # Create plot
  ggplot2::ggplot(cm_data, ggplot2::aes(x = Actual, y = Predicted, fill = IsCorrect)) +
    ggplot2::geom_tile(color = "white") +
    ggplot2::geom_text(ggplot2::aes(label = Label), fontface = "bold") +
    ggplot2::theme_minimal() +
    ggplot2::labs(title = title) +
    ggplot2::scale_fill_manual(values = c("TRUE" = charge_colors[1], "FALSE" = charge_colors[2])) +
    ggplot2::theme(legend.position = "none",
                 panel.grid = ggplot2::element_blank()) +
    my_theme
}

# Plot confusion matrices
p1 <- plot_confusion_matrix(svm_metrics$confusion, "SVM Confusion Matrix")
p2 <- plot_confusion_matrix(nn_metrics$confusion, "Neural Network Confusion Matrix")

# Display side by side
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

## Model Performance Comparison {#sec-performance-comparison}
```{r model-comparison}
# Create comparison table
model_comparison <- data.frame(
  Metric = c("Accuracy", "F1 Score", "Recall", "Precision", "AUROC", "AUPRC"),
  SVM = c(
    paste0(round(svm_metrics$accuracy * 100, 1), "%"),
    paste0(round(svm_metrics$f1 * 100, 1), "%"),
    paste0(round(svm_metrics$recall * 100, 1), "%"),
    paste0(round(svm_metrics$precision * 100, 1), "%"),
    round(svm_metrics$auroc, 3),
    round(svm_metrics$auprc, 3)
  ),
  NN = c(
    paste0(round(nn_metrics$accuracy * 100, 1), "%"),
    paste0(round(nn_metrics$f1 * 100, 1), "%"),
    paste0(round(nn_metrics$recall * 100, 1), "%"),
    paste0(round(nn_metrics$precision * 100, 1), "%"),
    round(nn_metrics$auroc, 3),
    round(nn_metrics$auprc, 3)
  ),
  Description = c(
    "",
    "for the \"high\" class",
    "(correctly identifying \"high\" cases)",
    "(accuracy of \"high\" predictions)",
    "",
    ""
  )
)

# Create numeric comparison table for highlighting
numeric_comparison <- data.frame(
  SVM = c(
    svm_metrics$accuracy,
    svm_metrics$f1,
    svm_metrics$recall,
    svm_metrics$precision,
    svm_metrics$auroc,
    svm_metrics$auprc
  ),
  NN = c(
    nn_metrics$accuracy,
    nn_metrics$f1,
    nn_metrics$recall,
    nn_metrics$precision,
    nn_metrics$auroc,
    nn_metrics$auprc
  )
)

# Create formatted table
kableExtra::kbl(
  model_comparison[, c("Metric", "SVM", "NN", "Description")],
  caption = "Model Performance Comparison",
  format = "latex",
  booktabs = TRUE,
  col.names = c("Metric", "SVM", "Neural Network", "Description"),
  align = c("l", "c", "c", "l")
) %>%
  kableExtra::kable_styling(
    latex_options = c("HOLD_position"),
    font_size = 9,
    position = "center"
  ) %>%
  kableExtra::row_spec(
    which(numeric_comparison$SVM > numeric_comparison$NN), 
    background = "#E1F5FE"
  ) %>%
  kableExtra::row_spec(
    which(numeric_comparison$NN > numeric_comparison$SVM), 
    background = "#E8F5E9"
  ) %>%
  kableExtra::footnote(
    general = "Color key: Blue = SVM performs better; Green = Neural Network performs better.",
    general_title = "Note: ",
    threeparttable = TRUE
  )
```

### Key Observations

1. The `r better_model` model achieves a higher F1 score of `r round(better_f1 * 100, 1)`%, indicating better overall balance between precision and recall.

2. The ROC curves show that both models have strong discriminative ability, with AUROCs of `r round(svm_metrics$auroc, 3)` for SVM and `r round(nn_metrics$auroc, 3)` for Neural Network.

3. The confusion matrices illustrate that both models make similar types of errors, but the `r better_model` has a slightly better balance between false positives and false negatives.

4. The `r ifelse(svm_metrics$recall > nn_metrics$recall, "SVM", "Neural Network")` model shows higher recall (`r round(max(svm_metrics$recall, nn_metrics$recall) * 100, 1)`%), meaning it's more effective at identifying true "high" charge cases.

5. The `r ifelse(svm_metrics$precision > nn_metrics$precision, "SVM", "Neural Network")` model has better precision (`r round(max(svm_metrics$precision, nn_metrics$precision) * 100, 1)`%), indicating fewer false positives.

# Feature Importance Analysis {#sec-feature-importance}

## SVM Feature Importance {#sec-svm-importance}

For SVM, we use permutation importance, which measures how much model performance decreases when each feature is randomly shuffled.

```{r svm-importance}
# Calculate permutation importance for SVM
set.seed(my_seed)
svm_importance <- function(model, X, y, n_repeats = 5) {
  # Get baseline performance
  baseline_pred <- predict(model, X)
  baseline_accuracy <- mean(baseline_pred == y)
  
  # Initialize importance scores
  feature_names <- colnames(X)
  importance_scores <- data.frame(
    Feature = feature_names,
    Importance = numeric(length(feature_names))
  )
  
  # Calculate importance for each feature
  for (feature in feature_names) {
    # Initialize importance for this feature
    feature_importance <- numeric(n_repeats)
    
    # Repeat permutation n_repeats times
    for (j in 1:n_repeats) {
      # Create permuted dataset
      X_permuted <- X
      X_permuted[, feature] <- sample(X_permuted[, feature])
      
      # Get predictions with permuted feature
      perm_pred <- predict(model, X_permuted)
      perm_accuracy <- mean(perm_pred == y)
      
      # Store importance
      feature_importance[j] <- baseline_accuracy - perm_accuracy
    }
    
    # Average importance across repeats
    importance_scores$Importance[importance_scores$Feature == feature] <- mean(feature_importance)
  }
  
  # Sort by importance
  importance_scores <- importance_scores %>%
    dplyr::arrange(desc(Importance))
  
  return(importance_scores)
}

# Calculate SVM importance
svm_imp_df <- svm_importance(final_svm_model, X_valid_scaled, y_valid)

```

## Neural Network Feature Importance {#sec-nn-importance}

For neural networks, we calculate permutation importance by measuring how much the model's performance decreases when each feature is permuted.

```{r nn-importance}
# Function to calculate permutation importance for neural network
calculate_nnet_importance <- function(model, X, y, n_repeats = 5) {
  # Get baseline performance
  baseline_pred <- predict(model, X)
  baseline_pred_class <- ifelse(baseline_pred > 0.5, 1, 0)
  baseline_accuracy <- mean(baseline_pred_class == y)
  
  # Initialize importance scores
  feature_names <- colnames(X)
  importance_scores <- data.frame(
    Feature = feature_names,
    Importance = numeric(length(feature_names))
  )
  
  # Calculate importance for each feature
  for (feature in feature_names) {
    # Initialize importance for this feature
    feature_importance <- numeric(n_repeats)
    
    # Repeat permutation n_repeats times
    for (i in 1:n_repeats) {
      # Create permuted dataset
      X_permuted <- X
      X_permuted[, feature] <- sample(X_permuted[, feature])
      
      # Get predictions with permuted feature
      perm_pred <- predict(model, X_permuted)
      perm_pred_class <- ifelse(perm_pred > 0.5, 1, 0)
      perm_accuracy <- mean(perm_pred_class == y)
      
      # Store importance
      feature_importance[i] <- baseline_accuracy - perm_accuracy
    }
    
    # Average importance across repeats
    importance_scores$Importance[importance_scores$Feature == feature] <- mean(feature_importance)
  }
  
  # Sort by importance
  importance_scores <- importance_scores %>%
    dplyr::arrange(desc(Importance))
  
  return(importance_scores)
}

# Calculate neural network importance
set.seed(my_seed)
nn_importance <- calculate_nnet_importance(final_nn_model, 
                                          X_valid_df, 
                                          valid_nnet$charges_numeric)

```

## Feature Importance Comparison {#sec-importance-comparison}

```{r feature-importance-plots, fig.cap="Neural Network Feature Importance", fig.height=3}
# Plot SVM importance
svm_plot <- ggplot2::ggplot(svm_imp_df %>% dplyr::slice(1:10), 
               ggplot2::aes(x = reorder(Feature, Importance), y = Importance)) +
  ggplot2::geom_bar(stat = "identity", fill = charge_colors[1]) +
  ggplot2::coord_flip() +
  ggplot2::labs(title = "SVM Feature Importance",
               x = "Feature",
               y = "Importance Score") +
  my_theme

# Plot NN importance
nn_plot <- ggplot2::ggplot(nn_importance %>% dplyr::slice(1:10), 
               ggplot2::aes(x = reorder(Feature, Importance), y = Importance)) +
  ggplot2::geom_bar(stat = "identity", fill = charge_colors[2]) +
  ggplot2::coord_flip() +
  ggplot2::labs(title = "NN Permutation Importance",
               x = "Feature",
               y = "Importance (Performance Decrease)") +
  my_theme

plots <- grid.arrange(svm_plot, nn_plot, ncol = 2, 
                     top = "Feature Importance Comparison: SVM vs Neural Network")
```

1. **Age**: Both models identify age as a significant factor, with the neural network giving it slightly more importance than the SVM model.

2. **Smoking Status**: Consistently ranks as a top predictor in both models, confirming our exploratory findings that smoking strongly influences insurance charges.

5. **Children**: Shows much lower importance compared to top 2 factors, but is still relevant to the classification task.

3. **BMI, Region and sex**: Shows moderate, varying degrees of importance in both models.

The agreement between both models on key predictors increases our confidence in these findings. The importance rankings align well with our exploratory data analysis, which showed strong associations between smoking status, age and insurance charges.

# Prediction on Test Data {#sec-prediction}

Based on our evaluation, we'll select the model that maximizes the F1 score for the "high" charges class for making predictions on the external test data.

```{r model-selection}
# Determine which model has better F1 score
selected_model <- ifelse(svm_metrics$f1 > nn_metrics$f1, "SVM", "Neural Network")
selected_f1 <- max(svm_metrics$f1, nn_metrics$f1)
selected_accuracy <- ifelse(selected_model == "SVM", svm_metrics$accuracy, nn_metrics$accuracy)

# Create a selection justification table
model_selection <- data.frame(
  "Selected_Model" = selected_model,
  "F1_Score" = round(selected_f1, 3),
  "Accuracy" = round(selected_accuracy, 3)
)

# Display selection table
kableExtra::kbl(
  model_selection,
  caption = "Model Selection for F1 Score Optimization",
  format = "latex",
  booktabs = TRUE,
  align = "c",
  col.names = c("Selected Model", "F1 Score", "Accuracy"),
  row.names = FALSE
) %>%
  kableExtra::kable_styling(
    latex_options = c("HOLD_position", "striped"),
    font_size = 9,
    position = "center",
    full_width = TRUE
  ) %>%
  kableExtra::column_spec(4, width = "8cm") %>%
  kableExtra::row_spec(0, bold = TRUE)
```

The `r selected_model` model achieves the highest F1 score, providing the best balance between precision and recall for identifying high-cost insurance cases.

```{r test-predictions}
# Convert test data to appropriate format
test_data_processed <- test_data %>%
  dplyr::mutate(
    sex = factor(sex, levels = levels(train_data$sex)),
    smoker = factor(smoker, levels = levels(train_data$smoker)),
    region = factor(region, levels = levels(train_data$region)),
    children = as.integer(children)
  )

# Make predictions using the selected model
if (selected_model == "SVM") {
  # For SVM
  test_pred_prob <- predict(final_svm_model, X_test_scaled, probability = TRUE)
  test_pred_prob <- attr(test_pred_prob, "probabilities")[, "high"]
  test_pred <- ifelse(test_pred_prob > 0.5, "high", "low")
  test_pred <- factor(test_pred, levels = c("low", "high"))
} else {
  # For Neural Network
  test_pred_prob <- predict(final_nn_model, as.data.frame(X_test_scaled))
  test_pred <- ifelse(test_pred_prob > 0.5, "high", "low")
  test_pred <- factor(test_pred, levels = c("low", "high"))
}

# Create output file
write.table(as.character(test_pred), 
            file = "TBSCES001.csv", 
            row.names = FALSE, 
            col.names = FALSE, 
            quote = FALSE)

# Calculate class distribution in predictions
pred_distribution <- table(test_pred)
pred_percents <- prop.table(pred_distribution) * 100

```

```{r prediction-visualization, fig.cap="Predicted Probabilities by Age and Smoking Status", fig.width=8, fig.height=5}
# Create visualization of predictions
pred_data <- data.frame(
  age = test_data_processed$age,
  bmi = test_data_processed$bmi,
  smoker = test_data_processed$smoker,
  region = test_data_processed$region,
  probability = test_pred_prob,
  prediction = test_pred
)

# Visualize predictions by age and smoking status
ggplot2::ggplot(pred_data, ggplot2::aes(x = age, y = probability, color = smoker, shape = prediction)) +
  ggplot2::geom_point(alpha = 0.7, size = 3) +
  ggplot2::geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray50") +
  ggplot2::facet_wrap(~ region) +
  ggplot2::scale_color_manual(values = c("no" = charge_colors[2], "yes" = charge_colors[1])) +
  ggplot2::labs(
    title = paste("Predicted Probabilities on External Test Data using", selected_model),
    subtitle = paste("Classification Threshold = 0.5, High Charges = ", round(pred_percents["high"], 1), "%, Low Charges = ", round(pred_percents["low"], 1), "%", sep=""),
    x = "Age",
    y = "Probability of High Charges",
    color = "Smoker",
    shape = "Prediction"
  ) +
  my_theme
```

The visualization of predicted probabilities reveals:

1. **Smoking Status**: Strong separation between smokers and non-smokers, with smokers consistently receiving higher probabilities of "high" charges

2. **Age**: Generally positive relationship with the probability of high charges, especially for non-smokers

3. **Regional Variations**: Some regional differences in predicted probabilities, with the southwest region showing slightly different patterns

4. **Decision Boundary**: The 0.5 threshold (dashed line) effectively separates the two classes

These patterns align with our feature importance analysis and exploratory findings, confirming that our model has captured meaningful relationships in the data.

# Conclusion {#sec-conclusion}

This analysis has developed and evaluated SVM and Neural Network models for predicting high versus low medical insurance costs based on patient characteristics.

1. **Model Performance**:
   - The `r selected_model` model achieved the best F1 score of `r round(selected_f1, 3)` for predicting high insurance charges
   - Both models showed strong discriminative ability with AUROC values above 0.85

2. **Practical Implications**:
   - **For Individuals**: Smoking cessation and weight management present the most significant opportunities for reducing insurance costs
   - **For Insurers**: Risk assessment models should incorporate these key factors and their interactions for more accurate premium setting
   - **For Policymakers**: Public health initiatives targeting smoking and obesity could have substantial impacts on healthcare costs